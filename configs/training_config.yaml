method: qlora  # or lora | full
epochs: 2
batch_size: 2
gradient_accumulation_steps: 4
learning_rate: 2e-4
max_seq_length: 512

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    - q_proj
    - v_proj

# QoL
logging_steps: 10
eval_steps: 50
save_steps: 100
save_total_limit: 2
seed: 42