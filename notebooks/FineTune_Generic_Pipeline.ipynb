{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "511b0387",
   "metadata": {},
   "source": [
    "# Fine‑Tuning Generic Pipeline (LoRA / QLoRA) — Colab\n",
    "\n",
    "This notebook:\n",
    "1) Installs dependencies  \n",
    "2) Fetches your project repo to `/content/project`  \n",
    "3) Reads YAML configs  \n",
    "4) Loads model (Kaggle Models or Hugging Face)  \n",
    "5) Validates dataset  \n",
    "6) Fine‑tunes with **LoRA/QLoRA**  \n",
    "7) Exports adapters + metadata  \n",
    "\n",
    "> **Tip:** If you see organization Drive policy errors, use a personal Gmail or run this on **Kaggle Notebooks**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa0d859",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install -q transformers datasets peft accelerate bitsandbytes pyyaml kagglehub jsonschema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e2a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =========================\n",
    "# PROJECT FETCHER (Git mode only; no OAuth, no Drive)\n",
    "# =========================\n",
    "GIT_URL    = \"https://github.com/<your-username>/<your-repo>.git\"  # <-- set this\n",
    "GIT_BRANCH = \"main\"      # change if you use another branch\n",
    "GIT_SUBDIR = \"\"          # optional: if your project is NOT at repo root, set e.g. \"generic-pipeline/\"\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "TARGET = \"/content/project\"\n",
    "\n",
    "# Clean target then clone\n",
    "os.system(f\"rm -rf {TARGET}\")\n",
    "if GIT_BRANCH:\n",
    "    os.system(f\"git clone --depth 1 --branch {GIT_BRANCH} {GIT_URL} {TARGET}\")\n",
    "else:\n",
    "    os.system(f\"git clone --depth 1 {GIT_URL} {TARGET}\")\n",
    "\n",
    "# If repo content is inside a subfolder, move it up into /content/project\n",
    "if GIT_SUBDIR:\n",
    "    src = os.path.join(TARGET, GIT_SUBDIR)\n",
    "    assert os.path.isdir(src), f\"Subdir '{GIT_SUBDIR}' not found in cloned repo\"\n",
    "    for name in os.listdir(src):\n",
    "        shutil.move(os.path.join(src, name), os.path.join(TARGET, name))\n",
    "    shutil.rmtree(os.path.join(TARGET, GIT_SUBDIR), ignore_errors=True)\n",
    "\n",
    "# basic structure checks\n",
    "must_dirs = [\"configs\", \"data\", \"scripts\"]\n",
    "missing = [d for d in must_dirs if not os.path.isdir(os.path.join(TARGET, d))]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Missing required folders in {TARGET}: {missing}\")\n",
    "\n",
    "print(\"[OK] Project ready at:\", TARGET)\n",
    "print(\"Contents:\", os.listdir(TARGET))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56a9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import yaml\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "from scripts.prepare_dataset import validate_or_raise\n",
    "from scripts.load_model_generic import load_model_and_tokenizer\n",
    "from scripts.export_adapters import export_adapters\n",
    "\n",
    "# Fallback if BASE_PATH not defined\n",
    "if \"BASE_PATH\" not in globals():\n",
    "    BASE_PATH = \"/content/project\" if os.path.exists(\"/content/project\") else \"/content\"\n",
    "\n",
    "print(\"Using BASE_PATH:\", BASE_PATH)\n",
    "\n",
    "\n",
    "def load_yaml(path):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Config not found: {path}\")\n",
    "    with open(path, \"r\") as f:\n",
    "        return yaml.safe_load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5ba3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_cfg = load_yaml(f\"{BASE_PATH}/configs/model_config.yaml\")\n",
    "train_cfg = load_yaml(f\"{BASE_PATH}/configs/training_config.yaml\")\n",
    "data_cfg = load_yaml(f\"{BASE_PATH}/configs/dataset_config.yaml\")\n",
    "usecase_cfg = load_yaml(f\"{BASE_PATH}/configs/usecase_config.yaml\")\n",
    "\n",
    "print(\"Configs loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a800b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from scripts.prepare_dataset import validate_or_raise\n",
    "\n",
    "def resolve_path(p: str) -> str:\n",
    "    \"\"\"Return absolute path under BASE_PATH unless already absolute.\"\"\"\n",
    "    return p if os.path.isabs(p) else f\"{BASE_PATH}/{p}\"\n",
    "\n",
    "# Resolve dataset paths (works for both relative and absolute)\n",
    "train_path = resolve_path(data_cfg[\"train_path\"])\n",
    "eval_path  = resolve_path(data_cfg[\"eval_path\"])\n",
    "\n",
    "# Validate files (basic JSONL schema checks)\n",
    "validate_or_raise(train_path)\n",
    "validate_or_raise(eval_path)\n",
    "\n",
    "# Load raw dataset\n",
    "dataset = load_dataset(\n",
    "    \"json\",\n",
    "    data_files={\n",
    "        \"train\": train_path,\n",
    "        \"eval\": eval_path,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Format messages into a single text per example\n",
    "def format_chat(example):\n",
    "    text = \"\"\n",
    "    for msg in example[\"messages\"]:\n",
    "        if msg[\"role\"] == \"user\":\n",
    "            # Keep the entire f-string on one line; use \\n for newlines\n",
    "            text += f\"### Instruction:\\n{msg['content']}\\n\"\n",
    "        elif msg[\"role\"] == \"assistant\":\n",
    "            text += f\"### Response:\\n{msg['content']}\\n\"\n",
    "    return {\"text\": text}\n",
    "\n",
    "dataset = dataset.map(format_chat)\n",
    "print(\"Dataset prepared. Columns:\", dataset[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d3949f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load model/tokenizer\n",
    "\n",
    "tokenizer, model, model_path = load_model_and_tokenizer(\n",
    "    model_source=model_cfg[\"model_source\"],\n",
    "    model_name=model_cfg[\"model_name\"],\n",
    "    quantization=model_cfg.get(\"quantization\", \"4bit\"),\n",
    "    device_map=model_cfg.get(\"device_map\", \"auto\"),\n",
    "    torch_dtype=model_cfg.get(\"torch_dtype\", \"bfloat16\"),\n",
    "    for_training=(train_cfg[\"method\"] in [\"lora\", \"qlora\"]),\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "if hasattr(model, \"config\"):\n",
    "    model.config.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# Optional: gradient checkpointing to reduce memory\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    try:\n",
    "        model.gradient_checkpointing_enable()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"Model loaded:\", model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5c322",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "MAX_LEN = train_cfg.get(\"max_seq_length\", 512)\n",
    "\n",
    "def tokenize(example):\n",
    "    return tokenizer(\n",
    "        example[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LEN,\n",
    "    )\n",
    "\n",
    "tokenized = dataset.map(\n",
    "    tokenize,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    ")\n",
    "print(\"Tokenization complete. Example keys:\", tokenized[\"train\"].column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ae96a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if train_cfg[\"method\"] in [\"lora\", \"qlora\"]:\n",
    "    lora_cfg = LoraConfig(\n",
    "        r=train_cfg[\"lora\"][\"r\"],\n",
    "        lora_alpha=train_cfg[\"lora\"][\"alpha\"],\n",
    "        target_modules=train_cfg[\"lora\"][\"target_modules\"],\n",
    "        lora_dropout=train_cfg[\"lora\"][\"dropout\"],\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "    )\n",
    "    model = get_peft_model(model, lora_cfg)\n",
    "    print(\"LoRA/QLoRA adapters attached.\")\n",
    "else:\n",
    "    print(\"Full finetuning mode.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7875d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/content/output\",\n",
    "    per_device_train_batch_size=train_cfg[\"batch_size\"],\n",
    "    gradient_accumulation_steps=train_cfg[\"gradient_accumulation_steps\"],\n",
    "    num_train_epochs=train_cfg[\"epochs\"],\n",
    "    learning_rate=float(train_cfg[\"learning_rate\"]),\n",
    "    logging_steps=train_cfg[\"logging_steps\"],\n",
    "    eval_steps=train_cfg[\"eval_steps\"],\n",
    "    save_steps=train_cfg[\"save_steps\"],\n",
    "    save_total_limit=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    fp16=True,\n",
    "    report_to=\"none\",\n",
    "    group_by_length=True,\n",
    ")\n",
    "\n",
    "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    eval_dataset=tokenized[\"eval\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13e1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "export_adapters(\n",
    "    model,\n",
    "    usecase_name=usecase_cfg[\"usecase_name\"],\n",
    "    extra_meta={\"model\": model_cfg[\"model_name\"]},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
