{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "511b0387",
      "metadata": {
        "id": "511b0387"
      },
      "source": [
        "# Fine‑Tuning Generic Pipeline (LoRA / QLoRA) — Colab\n",
        "\n",
        "This notebook:\n",
        "1) Installs dependencies  \n",
        "2) Fetches your project repo to `/content/project`  \n",
        "3) Reads YAML configs  \n",
        "4) Loads model (Kaggle Models or Hugging Face)  \n",
        "5) Validates dataset  \n",
        "6) Fine‑tunes with **LoRA/QLoRA**  \n",
        "7) Exports adapters + metadata  \n",
        "\n",
        "> **Tip:** If you see organization Drive policy errors, use a personal Gmail or run this on **Kaggle Notebooks**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "daa0d859",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "daa0d859",
        "outputId": "987c74de-f8a2-42e4-d87c-d6adf66e1c07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 60.7/60.7 MB 18.8 MB/s eta 0:00:00\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install -q transformers datasets peft accelerate bitsandbytes pyyaml kagglehub jsonschema\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "641e2a6d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "641e2a6d",
        "outputId": "2ad7ce6a-9fe9-48a3-b3d2-2083d1e1def3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Project ready at: /content/project\n",
            "Contents: ['configs', 'data', 'requirements.txt', 'notebooks', '.git', 'docs', 'README.md', 'scripts']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# =========================\n",
        "# PROJECT FETCHER (Git mode only; no OAuth, no Drive)\n",
        "# =========================\n",
        "GIT_URL    = \"https://github.com/Srienath2205/genericfinetuningpipeline.git\"  # <-- set this\n",
        "GIT_BRANCH = \"main\"      # change if you use another branch\n",
        "GIT_SUBDIR = \"\"          # optional: if your project is NOT at repo root, set e.g. \"generic-pipeline/\"\n",
        "\n",
        "import os, shutil\n",
        "\n",
        "TARGET = \"/content/project\"\n",
        "\n",
        "# Clean target then clone\n",
        "os.system(f\"rm -rf {TARGET}\")\n",
        "if GIT_BRANCH:\n",
        "    os.system(f\"git clone --depth 1 --branch {GIT_BRANCH} {GIT_URL} {TARGET}\")\n",
        "else:\n",
        "    os.system(f\"git clone --depth 1 {GIT_URL} {TARGET}\")\n",
        "\n",
        "# If repo content is inside a subfolder, move it up into /content/project\n",
        "if GIT_SUBDIR:\n",
        "    src = os.path.join(TARGET, GIT_SUBDIR)\n",
        "    assert os.path.isdir(src), f\"Subdir '{GIT_SUBDIR}' not found in cloned repo\"\n",
        "    for name in os.listdir(src):\n",
        "        shutil.move(os.path.join(src, name), os.path.join(TARGET, name))\n",
        "    shutil.rmtree(os.path.join(TARGET, GIT_SUBDIR), ignore_errors=True)\n",
        "\n",
        "# basic structure checks\n",
        "must_dirs = [\"configs\", \"data\", \"scripts\"]\n",
        "missing = [d for d in must_dirs if not os.path.isdir(os.path.join(TARGET, d))]\n",
        "if missing:\n",
        "    raise RuntimeError(f\"Missing required folders in {TARGET}: {missing}\")\n",
        "\n",
        "print(\"[OK] Project ready at:\", TARGET)\n",
        "print(\"Contents:\", os.listdir(TARGET))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "a56a9d0b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a56a9d0b",
        "outputId": "ddb300e8-0ae0-417e-c8fe-5819dadde215"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using BASE_PATH: /content/project\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import yaml\n",
        "from datasets import load_dataset\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from peft import get_peft_model, LoraConfig, TaskType\n",
        "import os, sys\n",
        "\n",
        "# Fallback if BASE_PATH not defined\n",
        "if \"BASE_PATH\" not in globals():\n",
        "    BASE_PATH = \"/content/project\" if os.path.exists(\"/content/project\") else \"/content\"\n",
        "\n",
        "# Ensure the project root is on sys.path so `import scripts.*` works\n",
        "if BASE_PATH not in sys.path:\n",
        "    sys.path.insert(0, BASE_PATH)\n",
        "\n",
        "# (Optional) sanity check\n",
        "assert os.path.isdir(os.path.join(BASE_PATH, \"scripts\")), f\"'scripts' folder not found under {BASE_PATH}\"\n",
        "print(\"Using BASE_PATH:\", BASE_PATH)\n",
        "\n",
        "\n",
        "from scripts.prepare_dataset import validate_or_raise\n",
        "from scripts.load_model_generic import load_model_and_tokenizer\n",
        "from scripts.export_adapters import export_adapters\n",
        "\n",
        "\n",
        "def load_yaml(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Config not found: {path}\")\n",
        "    with open(path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa5ba3ee",
      "metadata": {
        "id": "fa5ba3ee"
      },
      "outputs": [],
      "source": [
        "\n",
        "model_cfg = load_yaml(f\"{BASE_PATH}/configs/model_config.yaml\")\n",
        "train_cfg = load_yaml(f\"{BASE_PATH}/configs/training_config.yaml\")\n",
        "data_cfg = load_yaml(f\"{BASE_PATH}/configs/dataset_config.yaml\")\n",
        "usecase_cfg = load_yaml(f\"{BASE_PATH}/configs/usecase_config.yaml\")\n",
        "\n",
        "print(\"Configs loaded ✅\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "a800b347",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "a800b347",
        "outputId": "6f1b5ac6-9801-423e-c6ad-c474f5b96c3d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'data_cfg' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-318760509.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Resolve dataset paths (works for both relative and absolute)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0meval_path\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mresolve_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_cfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_path\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'data_cfg' is not defined"
          ]
        }
      ],
      "source": [
        "# --- Self-contained dataset load + format cell ---\n",
        "\n",
        "import os, sys, json\n",
        "import yaml\n",
        "from datasets import load_dataset\n",
        "\n",
        "# --- Ensure BASE_PATH and sys.path for `scripts` imports ---\n",
        "if \"BASE_PATH\" not in globals():\n",
        "    BASE_PATH = \"/content/project\" if os.path.exists(\"/content/project\") else \"/content\"\n",
        "if BASE_PATH not in sys.path:\n",
        "    sys.path.insert(0, BASE_PATH)\n",
        "\n",
        "# --- Safe imports of your project utilities ---\n",
        "from scripts.prepare_dataset import validate_or_raise  # requires sys.path to include BASE_PATH\n",
        "\n",
        "# --- Helper to load YAML (re-)if needed ---\n",
        "def load_yaml(path):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Config not found: {path}\")\n",
        "    with open(path, \"r\") as f:\n",
        "        return yaml.safe_load(f)\n",
        "\n",
        "# --- (Re)load configs if they are not in memory ---\n",
        "if \"data_cfg\" not in globals():\n",
        "    data_cfg = load_yaml(f\"{BASE_PATH}/configs/dataset_config.yaml\")\n",
        "if \"usecase_cfg\" not in globals():\n",
        "    try:\n",
        "        usecase_cfg = load_yaml(f\"{BASE_PATH}/configs/usecase_config.yaml\")\n",
        "    except FileNotFoundError:\n",
        "        usecase_cfg = {}\n",
        "\n",
        "print(\"Using BASE_PATH:\", BASE_PATH)\n",
        "\n",
        "# --- Path resolver works for both relative and absolute paths ---\n",
        "def resolve_path(p: str) -> str:\n",
        "    \"\"\"Return absolute path under BASE_PATH unless already absolute.\"\"\"\n",
        "    return p if os.path.isabs(p) else f\"{BASE_PATH}/{p}\"\n",
        "\n",
        "# Resolve dataset paths\n",
        "train_path = resolve_path(data_cfg[\"train_path\"])\n",
        "eval_path  = resolve_path(data_cfg[\"eval_path\"])\n",
        "\n",
        "# Validate files (basic JSONL schema checks)\n",
        "validate_or_raise(train_path)\n",
        "validate_or_raise(eval_path)\n",
        "\n",
        "# Load raw dataset\n",
        "dataset = load_dataset(\n",
        "    \"json\",\n",
        "    data_files={\n",
        "        \"train\": train_path,\n",
        "        \"eval\":  eval_path,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Format messages into a single text per example\n",
        "def format_chat(example):\n",
        "    text = \"\"\n",
        "    for msg in example[\"messages\"]:\n",
        "        if msg[\"role\"] == \"user\":\n",
        "            text += f\"### Instruction:\\n{msg['content']}\\n\"\n",
        "        elif msg[\"role\"] == \"assistant\":\n",
        "            text += f\"### Response:\\n{msg['content']}\\n\"\n",
        "    return {\"text\": text}\n",
        "\n",
        "dataset = dataset.map(format_chat)\n",
        "print(\"Dataset prepared. Columns:\", dataset[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9d3949f",
      "metadata": {
        "id": "f9d3949f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Load model/tokenizer\n",
        "\n",
        "tokenizer, model, model_path = load_model_and_tokenizer(\n",
        "    model_source=model_cfg[\"model_source\"],\n",
        "    model_name=model_cfg[\"model_name\"],\n",
        "    quantization=model_cfg.get(\"quantization\", \"4bit\"),\n",
        "    device_map=model_cfg.get(\"device_map\", \"auto\"),\n",
        "    torch_dtype=model_cfg.get(\"torch_dtype\", \"bfloat16\"),\n",
        "    for_training=(train_cfg[\"method\"] in [\"lora\", \"qlora\"]),\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "if hasattr(model, \"config\"):\n",
        "    model.config.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Optional: gradient checkpointing to reduce memory\n",
        "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
        "    try:\n",
        "        model.gradient_checkpointing_enable()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "print(\"Model loaded:\", model_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ad5c322",
      "metadata": {
        "id": "8ad5c322"
      },
      "outputs": [],
      "source": [
        "\n",
        "MAX_LEN = train_cfg.get(\"max_seq_length\", 512)\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LEN,\n",
        "    )\n",
        "\n",
        "tokenized = dataset.map(\n",
        "    tokenize,\n",
        "    remove_columns=dataset[\"train\"].column_names,\n",
        ")\n",
        "print(\"Tokenization complete. Example keys:\", tokenized[\"train\"].column_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ae96a4",
      "metadata": {
        "id": "76ae96a4"
      },
      "outputs": [],
      "source": [
        "\n",
        "if train_cfg[\"method\"] in [\"lora\", \"qlora\"]:\n",
        "    lora_cfg = LoraConfig(\n",
        "        r=train_cfg[\"lora\"][\"r\"],\n",
        "        lora_alpha=train_cfg[\"lora\"][\"alpha\"],\n",
        "        target_modules=train_cfg[\"lora\"][\"target_modules\"],\n",
        "        lora_dropout=train_cfg[\"lora\"][\"dropout\"],\n",
        "        task_type=TaskType.CAUSAL_LM,\n",
        "    )\n",
        "    model = get_peft_model(model, lora_cfg)\n",
        "    print(\"LoRA/QLoRA adapters attached.\")\n",
        "else:\n",
        "    print(\"Full finetuning mode.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22d7875d",
      "metadata": {
        "id": "22d7875d"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/output\",\n",
        "    per_device_train_batch_size=train_cfg[\"batch_size\"],\n",
        "    gradient_accumulation_steps=train_cfg[\"gradient_accumulation_steps\"],\n",
        "    num_train_epochs=train_cfg[\"epochs\"],\n",
        "    learning_rate=float(train_cfg[\"learning_rate\"]),\n",
        "    logging_steps=train_cfg[\"logging_steps\"],\n",
        "    eval_steps=train_cfg[\"eval_steps\"],\n",
        "    save_steps=train_cfg[\"save_steps\"],\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    fp16=True,\n",
        "    report_to=\"none\",\n",
        "    group_by_length=True,\n",
        ")\n",
        "\n",
        "collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"eval\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=collator,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "print(\"Training complete.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b13e1f1d",
      "metadata": {
        "id": "b13e1f1d"
      },
      "outputs": [],
      "source": [
        "\n",
        "export_adapters(\n",
        "    model,\n",
        "    usecase_name=usecase_cfg[\"usecase_name\"],\n",
        "    extra_meta={\"model\": model_cfg[\"model_name\"]},\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}